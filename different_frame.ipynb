{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('fordTrain.csv')\n",
    "df1=pd.read_csv('fordTest.csv')\n",
    "df=df.drop(['TrialID','ObsNum','P8','V7','V9'],axis=1) \n",
    "df1=df1.drop(['TrialID','ObsNum','P8','V7','V9'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('fordTrain.csv')\n",
    "df1=pd.read_csv('fordTest.csv')\n",
    "df=df.drop(['TrialID','ObsNum','P8','V7','V9'],axis=1) \n",
    "df1=df1.drop(['TrialID','ObsNum','P8','V7','V9'],axis=1)\n",
    "y_train=df['IsAlert']  # The labels for the train set\n",
    "#X_train=df.drop(['IsAlert'],axis=1) # The features of the train set\n",
    "\n",
    "y_test=df1['IsAlert'] # The labels for the test set\n",
    "#X_test=df1.drop(['IsAlert'],axis=1) # The features of the test set\n",
    "X_train=df.drop(['IsAlert','P1','P2','P3','P4','P5','E1','E2','E3','E3','E4','E5','E6','E9','E11','V1','V2','V3','V4','V5','V6','V8','V10'],axis=1)\n",
    "X_test=df1.drop(['IsAlert','P1','P2','P3','P4','P5','E1','E2','E3','E3','E4','E5','E6','E9','E11','V1','V2','V3','V4','V5','V6','V8','V10'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "#Gradient Boosting\n",
    "model1 = GradientBoostingClassifier(criterion='friedman_mse',\n",
    "                                    learning_rate= 0.5,\n",
    "                                    loss= 'deviance', \n",
    "                                    max_depth= 8, \n",
    "                                    n_estimators= 100)\n",
    "\n",
    "#model1 = model1.fit(X_train,y_train)\n",
    "\n",
    "#XGBoost\n",
    "model2 = XGBClassifier(booster='gbtree',\n",
    "                       gamma= 0, \n",
    "                       learning_rate= 0.7,\n",
    "                       max_depth= 8,\n",
    "                       n_estimators= 400, \n",
    "                       objective= 'binary:logistic')\n",
    "\n",
    "#model2 = model2.fit(X_train,y_train)\n",
    "\n",
    "#Random Forest\n",
    "model3 = RandomForestClassifier(criterion= 'gini',\n",
    "                                max_depth = 100,\n",
    "                                min_samples_leaf= 1,\n",
    "                                min_samples_split= 2,\n",
    "                                n_estimators= 20)\n",
    "\n",
    "#model3 = model3.fit(X_train,y_train)\n",
    "\n",
    "#lightGBM\n",
    "model4 = LGBMClassifier(boosting_type = 'gbdt',\n",
    "                        learning_rate = 0.3,\n",
    "                        max_depth = 7,\n",
    "                        n_estimators = 400,\n",
    "                        num_leaves = 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('GBC', 0.955660355057561)\n",
      "('XGB', 0.9754036356759568)\n",
      "('RF', 0.9827151738647378)\n",
      "('LGBM', 0.9460463796893838)\n"
     ]
    }
   ],
   "source": [
    "def classification_crossvalscore(model, x, y):\n",
    "    acc = cross_val_score(model, x, y, scoring = 'accuracy', cv = kfold )\n",
    "    #prec = cross_val_score(model, x, y, scoring = 'precision', cv = 5)\n",
    "    #recall = cross_val_score(model, x, y, scoring = 'recall', cv = 5)\n",
    "    #rocauc = cross_val_score(model, x, y, scoring = 'roc_auc', cv = 5)\n",
    "    return acc#, prec, recall, rocauc\n",
    "kfold = KFold(n_splits=3, shuffle=False, random_state=42)\n",
    "for model,label in zip([model1,model2,model3, model4],[\"GBC\",\"XGB\", \"RF\", \"LGBM\"]):\n",
    "              print(label,classification_crossvalscore(model,X_train,y_train).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict model and stack model\n",
    "GBC_pred = model1.fit(X_train,y_train).predict(X_test)\n",
    "XGB_pred = model2.fit(X_train,y_train).predict(X_test)\n",
    "RF_pred = model3.fit(X_train,y_train).predict(X_test)\n",
    "LGB_pred = model4.fit(X_train,y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out Sample scoring-Standalone\n",
      "('GBC', 0.9562891520695932)\n",
      "('XGB', 0.9754036356759568)\n",
      "('RF', 0.9826537125026592)\n",
      "('LGBM', 0.9460463796893838)\n"
     ]
    }
   ],
   "source": [
    "print(\"Out Sample scoring-Standalone\")\n",
    "\n",
    "\n",
    "for model,label in zip([model1,model2,model3, model4],[\"GBC\",\"XGB\", \"RF\", \"LGBM\"]):\n",
    "              print(label,classification_crossvalscore(model,X_train,y_train).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [2]\n",
      "metric:       [accuracy_score]\n",
      "mode:         [oof_pred_bag]\n",
      "n_models:     [3]\n",
      "\n",
      "model  0:     [GradientBoostingClassifier]\n",
      "    fold  0:  [0.95664137]\n",
      "    fold  1:  [0.95666265]\n",
      "    fold  2:  [0.95590384]\n",
      "    ----\n",
      "    MEAN:     [0.95640262] + [0.00035280]\n",
      "    FULL:     [0.95640262]\n",
      "\n",
      "model  1:     [XGBClassifier]\n",
      "    fold  0:  [0.97475356]\n",
      "    fold  1:  [0.97421459]\n",
      "    fold  2:  [0.97454790]\n",
      "    ----\n",
      "    MEAN:     [0.97450535] + [0.00022208]\n",
      "    FULL:     [0.97450535]\n",
      "\n",
      "model  2:     [RandomForestClassifier]\n",
      "    fold  0:  [0.98226367]\n",
      "    fold  1:  [0.98227076]\n",
      "    fold  2:  [0.98263953]\n",
      "    ----\n",
      "    MEAN:     [0.98239132] + [0.00017553]\n",
      "    FULL:     [0.98239132]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from vecstack import stacking\n",
    "\n",
    "#Initialize first level\n",
    "models = [model1, model2, model3] #input models as a list for metamodel\n",
    "\n",
    "#Performing Stacking\n",
    "S_train, S_test = stacking(models,                     # list of models\n",
    "                           X_train, y_train, X_test,   # data\n",
    "                           regression=False,            # regression task (if you need  classification - set to False)\n",
    "                           mode='oof_pred_bag',        # mode: oof for train set, predict test  //set in each fold and find mean\n",
    "                           save_dir=None,              # do not save result and log (to save in current dir - set to '.')\n",
    "                           metric=accuracy_score,      # metric: callable\n",
    "                           n_folds=3,                  # number of folds\n",
    "                           shuffle=True,               # shuffle the data\n",
    "                           random_state=42,             # ensure reproducibility\n",
    "                           verbose=2)                  # print all info \"\"\"\n",
    "\n",
    "\n",
    "# Initialize 2nd level model  with XGB\n",
    "model = model4\n",
    "    \n",
    "# Fit 2nd level model\n",
    "stack = model.fit(S_train, y_train)\n",
    "\n",
    "# Predict\n",
    "#y_pred = model.predict(S_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Sample scoring-Vecstack\n",
      "('GBC', 0.9566791953289364)\n",
      "('XGB', 0.9754036356759568)\n",
      "('RF', 0.9824220504455949)\n",
      "('Vecstack', 0.9460463796893838)\n"
     ]
    }
   ],
   "source": [
    "print(\"In Sample scoring-Vecstack\")\n",
    "\n",
    "\n",
    "for model,label in zip([model1,model2,model3, stack],[\"GBC\",\"XGB\", \"RF\", \"Vecstack\"]):\n",
    "              print(label,classification_crossvalscore(model,X_train,y_train).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict model and stack model\n",
    "GBC_pred = model1.fit(X_train,y_train).predict(X_test)\n",
    "XGB_pred = model2.fit(X_train,y_train).predict(X_test)\n",
    "RF_pred = model3.fit(X_train,y_train).predict(X_test)\n",
    "stack_pred = stack.predict(S_test)  #has to be on the S_test not on X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of Sample scoring-Vecstack\n",
      "('GBC', 0.9599722006188671)\n",
      "('XGB', 0.9808713782205086)\n",
      "('RF', 0.9883893457768659)\n",
      "('Vecstack', 0.9872144909789905)\n"
     ]
    }
   ],
   "source": [
    "#accuracy score out of sample\n",
    "print(\"Out of Sample scoring-Vecstack\")\n",
    "\n",
    "for model, label in zip([GBC_pred, XGB_pred, RF_pred, stack_pred],[\"GBC\",\"XGB\", \"RF\", \"Vecstack\"]):\n",
    "    print(label,accuracy_score(y_test, model, normalize = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [('GBC', model1),('XGB', model2),('RF', model3), ('LGBM', model4)]\n",
    "\n",
    "#Voting Classifier\n",
    "ecf = VotingClassifier(estimators= estimators,\n",
    "                          voting='soft',            #soft for probability\n",
    "                          weights=None,\n",
    "                          n_jobs=1,\n",
    "                          flatten_transform=None\n",
    "                          )\n",
    "\n",
    "\n",
    "#Voting Classifier fit\n",
    "vc = ecf.fit(X_train, y_train)\n",
    "\n",
    "#Voting Classifier predict\n",
    "VC_pred=vc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Sample scoring-Voting Classifier\n",
      "('GBC', 0.9563837080112522)\n",
      "('XGB', 0.9754036356759568)\n",
      "('RF', 0.9825615204595418)\n",
      "('LGBM', 0.9460463796893838)\n",
      "('VC_stack', 0.9734108692054937)\n"
     ]
    }
   ],
   "source": [
    "print(\"In Sample scoring-Voting Classifier\")\n",
    "\n",
    "for model,label in zip([model1,model2,model3,model4, vc],[\"GBC\",\"XGB\", \"RF\",\"LGBM\" ,\"VC_stack\"]):\n",
    "              print(label,classification_crossvalscore(model,X_train,y_train).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of Sample scoring-Voting Classifier\n",
      "('GBC', 0.9599722006188671)\n",
      "('XGB', 0.9808713782205086)\n",
      "('RF', 0.9883893457768659)\n",
      "('LGBM', 0.9501321022178831)\n",
      "('VC_stack', 0.9783727433686893)\n"
     ]
    }
   ],
   "source": [
    "#accuracy score out of sample\n",
    "print(\"Out of Sample scoring-Voting Classifier\")\n",
    "\n",
    "\n",
    "for model, label in zip([GBC_pred, XGB_pred, RF_pred,LGB_pred,VC_pred],[\"GBC\",\"XGB\", \"RF\",\"LGBM\" ,\"VC_stack\"]):\n",
    "    print(label,accuracy_score(y_test, model, normalize = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.classifier import StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models classifier\n",
    "classifiers = [model1,model2,model3]\n",
    "\n",
    "#stacking classifier\n",
    "stack_c = StackingClassifier(classifiers = classifiers,\n",
    "                            meta_classifier=model4)\n",
    "\n",
    "#mlxtend classifier fit\n",
    "stack = stack_c.fit(X_train, y_train)\n",
    "\n",
    "#mlxtend classifier predict\n",
    "mlx_pred=stack.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Sample scoring-mlxtend Classifier\n",
      "('GBC', 0.9562655130841785)\n",
      "('XGB', 0.9754036356759568)\n",
      "('RF', 0.982511878590171)\n",
      "('mlx_stack', 0.9735597948136067)\n"
     ]
    }
   ],
   "source": [
    "print(\"In Sample scoring-mlxtend Classifier\")\n",
    "\n",
    "for model,label in zip([model1,model2,model3, vc],[\"GBC\",\"XGB\", \"RF\", \"mlx_stack\"]):\n",
    "              print(label,classification_crossvalscore(model,X_train,y_train).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of Sample scoring-mlxtend Classifier\n",
      "('GBC', 0.9599722006188671)\n",
      "('XGB', 0.9501321022178831)\n",
      "('RF', 0.9883893457768659)\n",
      "('mlx_stack', 0.9884500190293383)\n"
     ]
    }
   ],
   "source": [
    "#accuracy score out of sample\n",
    "print(\"Out of Sample scoring-mlxtend Classifier\")\n",
    "\n",
    "for model, label in zip([GBC_pred, LGB_pred, RF_pred, mlx_pred],[\"GBC\",\"XGB\", \"RF\", \"mlx_stack\"]):\n",
    "    print(label,accuracy_score(y_test, model, normalize = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
